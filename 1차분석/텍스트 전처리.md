# 텍스트 전처리 공부



> 현재 텍스트(소설)을 이용해서 인물의 관계를 보여주는 앱을 개발하는 프로젝트를 진행 중이다. 형태소 분석을 한 후에, TF-IDF 등 sklearn의 라이브러리를 이용할 계획이다. 
>
> 그런데 차질이 생겼다. TF-IDF에서 전체 문서의 수가 너무 많이 나오게 되었다. 코드를 보고, 출력을 하니 '...'이 사이에 들어간 모든 문장들을 하나의 문서로 취급을 한다. 이를 바꿔주는 코드를 짜려고 하니, 지식이 부족하다.
>
> 그리해서 전처리를 제대로 하기 위해서 공부를 시작한다. 대부분의 내용은  [링크](https://wikidocs.net/book/2155) 를 참고했다. 링크의 내용은 영어 텍스트 마이닝부분이 많아서, 나는 한글을 이용한 텍스트 마이닝에 필요한 지식만 간추렸다. 거기다가, 모르는 부분이 있으면, 따로 선생님께 질문, 고수분께 질문한 부분이 첨부되어 있다.
>
> 현재까지의 내 생각은 삽질은 어느정도는 필요하다. 하지만, 너무 많이 하면 뻘짓이 되고, 시간은 기회비용이니 이 점을 생각하면서 공부를 하면 좋겠다. 모든 분들이 도움이 되기를...Good luck 

 

## 1 토큰화(Tokenization)

문서에는 문자들 말고도 ! , '' " . " " 등과 같은 기호들도 있습니다. 이러한 기호들 때문에 분석중에 오류가 나거나, 정확한 수치가 나오지 않게 됩니다. 그래서 분석을 하기 전에 기호들을 처리해주게 됩니다. 처리(정제)가 완료된 문서는 아래와 같이 나옵니다.

```
입력: Time is an illusion. Lunchtime double so!
출력 : "Time", "is", "an", "illustion", "Lunchtime", "double", "so"
```

하지만, 정제작업은 그저 제외를 한다고 생각만 해서는 안됩니다. 왜냐하면온점(.)과 같은 경우는 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때, 온점(.)을 제외하지 않을 수 있습니다.

또 다른 예를 들어보면, 단어 자체에서 구두점을 갖고 있는 경우도 있는데, m.p.h나 Ph.D나 AT&T 같은 경우가 있습니다. 또 특수 문자의 달러(![img](https://wikidocs.net/images/page/21693/%EB%8B%AC%EB%9F%AC.PNG))나 슬래시(/)로 예를 들어보면, $45.55와 같은 가격을 의미 하기도 하고, 01/02/06은 날짜를 의미하기도 합니다. 보통 이런 경우 45.55를 하나로 취급해야하지, 45와 55로 따로 분류하고 싶지는 않을 것입니다.

숫자 사이에 컴마(,)가 들어가는 경우도 있습니다. 가령 보통 수치를 표현할 때는 123,456,789와 같이 세 자리 단위로 컴마가 들어갑니다.



### 1-1. 한국어 토크나이즈

 **영어와는 달리 한국어에는 조사라는 것이 존재합니다**. (예: '그가', '그에게', '그를', '그와', '그는')그래서 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘들고 번거로워지는 경우가 많습니다.  

 **한국어는** 어절이 독립적인 단어로 구성되는 것이 아니라 **조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다는 의미입니다.**

> 1. 조사가 붙어있다.
> 2. 한국어는 띄어쓰기가 영어만큼 잘 지켜지지 않는다.



### 1-2 실습

```
from konlpy.tag import Komoran

komoran=Komoran() 

text_file=open("C"/경로/text1,'r',encoding='utf-8')
data=text_file.readlines()
noun_data = komoran.nouns(data)
print(noun_data)

결과
['북녘', '동포', '여러분', '남녘', '국민', '여러분', '해외', '동포', '여러분', '전쟁', '한반도', '시작', '남과 북', '오늘', '한반'......]
```





## 2. 정제와 정규화



정제를 위해서 2가지를 제거 한다.

1. 등장 빈도가 적은 단어
2. 길이가 짧은 단어(영어에 더 적합함.)



### 2-1. 예시

```
import re
text = "I was wondering if anyone out there could enlighten me on this car."
shortword = re.compile(r'\W*\b\w{1,2}\b')
print(shortword.sub('', text))

결과: was wondering anyone out there could enlighten this car.
```





## 3. 표제어 추출(Lemmatization)/어간추출(Stemming)

위 두개의 추출 방식은 코퍼스(말풍치)에 있는 같은 의미를 가지고 있는 단어를 줄일 수 있는 방식이다. 





### 3-1. 표제어 추출

표제어는 다른 형태를 가지고 있더라도, 그 뿌리 단어를 찾아가서 단어의 개수를 줄일 수 있는지 판단합니다. 예를 들어서 am are is 등은 be에서 파생된 것 처럼 말이죠. 거래서 표제어 추출은 단어를 형태학적 파싱을 진행하는 것입니다. 형태소란, 의미를 가진 가장 작은 단위를 뜻하는데, 두가지 종류가 있습니다.

1. 어간(stem)
   :단어의 의미를 담고 있는 단어의 핵심 부분.
2. 접사(affix)
   : 단어에 추가적인 의미를 주는 부분.

예: cats = cat(어간) + -s(접사)



하지만, 1가지 단점이 있습니다. 표제어는 본래 단어의 품사 정보를 알아야만, 정확한 결과를 얻을 수 있습니다.  만약 그렇지 않다면, 아래와 같은 에러가 발생합니다.

```
입력: words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting'] print([n.lemmatize(w) for w in words])

출력: ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']
```

그래서 이러한 에러를 막기 위해서 품사를 알려주는 기능을 가지고 있는  WordNetLemmatizer 를 사용해야 합니다.

 어 추출은 문맥을 고려하며, 수행했을 때의 결과는 해당 단어의 품사 정보를 보존합니다. (POS 태그를 보존한다고도 말할 수 있습니다.) 



### 3-2. 어간 추출(Stemming)

어간 추출을 한 결과는 사전에 존재하지 않는 단어일 경우가 많습니다.  바로 예시를 봅니다.

```
입력 

text="This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."
words=word_tokenize(text)
print(words)

['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', "'s", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']

결과

['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', "'s", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']


```

>   **Porter 알고리즘**  참고



### 3-3. 그래서 한국어는? 한국어 어간 추출

한국어는 5언 9품사의 구조를 가지고 있습니다. 그 중, 용언의 동사와 형용사가 어간과 어미의 결합으로 구성이 됩니다. 



#### 3-3-1 활용(conjugation)

활용은 한국어 외에도 다른 국가의 언어에서도 사용이 됩니다. **활용은 어간(stem)이 어미(ending)을  가지는 일을 말합니다**

**어간(stem)** : 용언(동사, 형용사)을 활용할 때, 원칙적으로 모양이 변하지 않는 부분. 활용에서 어미에 선행하는 부분. 때론 어간의 모양도 바뀔 수 있음(예: 긋다, 긋고, 그어서, 그어라).

**어미(ending)**: 용언의 어간 뒤에 붙어서 활용하면서 변하는 부분이며, 여러 문법적 기능을 수행

활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉩니다. 



## 4. 한국에서 불용어 제거하기

 한국어에서 불용어를 제거하는 방법으로는 간단하게는 토큰화 후에 조사, 접속사 등을 제거하는 방법이 있습니다. 하지만 불용어를 제거하려고 하다보면 조사나 접속사와 같은 단어들뿐만 아니라 명사, 형용사와 같은 단어들 중에서 불용어로서 제거하고 싶은 단어들이 생기기도 합니다. 결국에는 사용자가 직접 불용어 사전을 만들게 되는 경우가 많습니다. 이번에는 직접 불용어를 정의해보고, 주어진 문장으로부터 직접 정의한 불용어 사전을 참고로 불용어를 제거해보겠습니다. 

```
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든"
# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님
stop_words=stop_words.split(' ')
word_tokens = word_tokenize(example)

result = [] 
for w in word_tokens: 
    if w not in stop_words: 
        result.append(w) 
# 위의 4줄은 아래의 한 줄로 대체 가능
# result=[word for word in word_tokens if not word in stop_words]

print(word_tokens) 
print(result)


['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']
['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']
```





## 5. 정규 표현식 

정규 표현식(영어: regular expression) 또는 정규식(正規式)은 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어이다. 파이썬에서는 정규 표현식 모듈 re을 지원하므로, 이를 이용하면 특정 규칙이 있는 텍스트 데이터를 빠르게 정제할 수 있습니다.  



### 5-1. 정규 표현식의 기초, 메타 문자

메타문자: 원래 그 문자가 가진 뜻이 아닌 특별한 용도로 사용하는 문자

```
. ^ $ * + ? { } [ ] \ | ( )
```



### 5-2. 문자 클래스 [ ]



자 클래스로 만들어진 정규식은 `"[ ] 사이의 문자들과 매치"`라는 의미를 갖는다.  문자 클래스를 만드는 메타 문자인 [ ] 사이에는 어떤 문자도 들어갈 수 있다. 

즉 정규 표현식이 [abc]라면 이 표현식의 의미는 "a, b, c 중 한 개의 문자와 매치"를 뜻한다. 이해를 돕기 위해 문자열 "a", "before", "dude"가 정규식 [abc]와 어떻게 매치되는지 살펴보자.



- "a"는 정규식과 일치하는 문자인 "a"가 있으므로 매치
- "before"는 정규식과 일치하는 문자인 "b"가 있으므로 매치
- "dude"는 정규식과 일치하는 문자인 a, b, c 중 어느 하나도 포함하고 있지 않으므로 매치되지 않음



[ ] 안의 두 문자 사이에 하이픈(-)을 사용하면 두 문자 사이의 범위(From - To)를 의미한다. 예를 들어 [a-c]라는 정규 표현식은 [abc]와 동일하고 [0-5]는 [012345]와 동일하다.

다음은 하이픈(-)을 사용한 문자 클래스의 사용 예이다.

- [a-zA-Z] : 알파벳 모두
- [0-9] : 숫자



한가지 유의하고 쓰면 좋은 문자하나가 있다. 그것은 바로 `^`인데**, 문자 클래스 안에 `^` 메타 문자를 사용할 경우에는 반대(not)라는 의미를 갖는다.** 예를 들어 `[^0-9]`라는 정규 표현식은 숫자가 아닌 문자만 매치된다. 

```
[자주 사용하는 문자 클래스]

[0-9] 또는 [a-zA-Z] 등은 무척 자주 사용하는 정규 표현식이다. 이렇게 자주 사용하는 정규식은 별도의 표기법으로 표현할 수 있다. 다음을 기억해 두자.

\d - 숫자와 매치, [0-9]와 동일한 표현식이다.
\D - 숫자가 아닌 것과 매치, [^0-9]와 동일한 표현식이다.
\s - whitespace 문자와 매치, [ \t\n\r\f\v]와 동일한 표현식이다. 맨 앞의 빈 칸은 공백문자(space)를 의미한다.
\S - whitespace 문자가 아닌 것과 매치, [^ \t\n\r\f\v]와 동일한 표현식이다.
\w - 문자+숫자(alphanumeric)와 매치, [a-zA-Z0-9_]와 동일한 표현식이다.
\W - 문자+숫자(alphanumeric)가 아닌 문자와 매치, [^a-zA-Z0-9_]와 동일한 표현식이다.

대문자로 사용된 것은 소문자의 반대임을 추측할 수 있다.
```



### 5-3. Dot(.)

 정규 표현식의 Dot(.) 메타 문자는 줄바꿈 문자인 `\n`을 제외한 모든 문자와 매치됨을 의미한다. 

```
a.b == "a + 모든 문자 + b"

즉 a와 b라는 문자 사이에 어떤 문자가 들어가도 모두 매치된다는 의미이다.
```



**예**

 "aab"는 가운데 문자 "a"가 모든 문자를 의미하는 `.`과 일치하므로 정규식과 매치된다. 

 "a0b"는 가운데 문자 "0"가 모든 문자를 의미하는 `.`과 일치하므로 정규식과 매치된다. 



**???**

 "abc"는 "a"문자와 "b"문자 사이에 어떤 문자라도 하나는있어야 하는 이 정규식과 일치하지 않으므로 매치되지 않는다. 

### 5-4 반복(*)

```
ca*t
```

여기서  ***** 는 반복을 의미하는 것으로, * 바로 앞에 있는 문자 a가 무한대로 번복될 수 있다는 의미를 가진다. 그래서 아래와 같은 문자열과 매치가 됩니다.

| 정규식 | 문자열 | Match 여부 | 설명                                    |
| :----- | :----- | :--------- | :-------------------------------------- |
| `ca*t` | ct     | Yes        | "a"가 0번 반복되어 매치                 |
| `ca*t` | cat    | Yes        | "a"가 0번 이상 반복되어 매치 (1번 반복) |
| `ca*t` | caaat  | Yes        | "a"가 0번 이상 반복되어 매치 (3번 반복) |



### 5-5. 반복(+)






















# 텍스트 전처리 공부



> 현재 텍스트(소설)을 이용해서 인물의 관계를 보여주는 앱을 개발하는 프로젝트를 진행 중이다. 형태소 분석을 한 후에, TF-IDF 등 sklearn의 라이브러리를 이용할 계획이다. 
>
> 그런데 차질이 생겼다. TF-IDF에서 전체 문서의 수가 너무 많이 나오게 되었다. 코드를 보고, 출력을 하니 '...'이 사이에 들어간 모든 문장들을 하나의 문서로 취급을 한다. 이를 바꿔주는 코드를 짜려고 하니, 지식이 부족하다.
>
> 그리해서 전처리를 제대로 하기 위해서 공부를 시작한다.



## 1 토큰화(Tokenization)

문서에는 문자들 말고도 ! , '' " . " " 등과 같은 기호들도 있습니다. 이러한 기호들 때문에 분석중에 오류가 나거나, 정확한 수치가 나오지 않게 됩니다. 그래서 분석을 하기 전에 기호들을 처리해주게 됩니다. 처리(정제)가 완료된 문서는 아래와 같이 나옵니다.

```
입력: Time is an illusion. Lunchtime double so!
출력 : "Time", "is", "an", "illustion", "Lunchtime", "double", "so"
```

하지만, 정제작업은 그저 제외를 한다고 생각만 해서는 안됩니다. 왜냐하면온점(.)과 같은 경우는 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때, 온점(.)을 제외하지 않을 수 있습니다.

또 다른 예를 들어보면, 단어 자체에서 구두점을 갖고 있는 경우도 있는데, m.p.h나 Ph.D나 AT&T 같은 경우가 있습니다. 또 특수 문자의 달러(![img](https://wikidocs.net/images/page/21693/%EB%8B%AC%EB%9F%AC.PNG))나 슬래시(/)로 예를 들어보면, $45.55와 같은 가격을 의미 하기도 하고, 01/02/06은 날짜를 의미하기도 합니다. 보통 이런 경우 45.55를 하나로 취급해야하지, 45와 55로 따로 분류하고 싶지는 않을 것입니다.

숫자 사이에 컴마(,)가 들어가는 경우도 있습니다. 가령 보통 수치를 표현할 때는 123,456,789와 같이 세 자리 단위로 컴마가 들어갑니다.



### 1-1. 한국어 토크나이즈

 **영어와는 달리 한국어에는 조사라는 것이 존재합니다**. (예: '그가', '그에게', '그를', '그와', '그는')그래서 자연어 처리를 하다보면 같은 단어임에도 서로 다른 조사가 붙어서 다른 단어로 인식이 되면 자연어 처리가 힘들고 번거로워지는 경우가 많습니다.  

 **한국어는** 어절이 독립적인 단어로 구성되는 것이 아니라 **조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다는 의미입니다.**

> 1. 조사가 붙어있다.
> 2. 한국어는 띄어쓰기가 영어만큼 잘 지켜지지 않는다.



### 1-2 실습

```
from konlpy.tag import Komoran

komoran=Komoran() 

text_file=open("C"/경로/text1,'r',encoding='utf-8')
data=text_file.readlines()
noun_data = komoran.nouns(data)
print(noun_data)


```

















